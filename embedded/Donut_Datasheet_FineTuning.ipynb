{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting check_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile check_setup.py\n",
    "\"\"\"\n",
    "Pre-flight Check Script (Final Fix - Solves Double Counting & sample_id)\n",
    "========================================================================\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# GLOBAL VARIABLES\n",
    "# ============================================================================\n",
    "IMAGES_DIR = None\n",
    "JSONL_ROOT_DIR = None\n",
    "\n",
    "def print_section(title):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "def check_dependencies():\n",
    "    print_section(\"CHECKING DEPENDENCIES\")\n",
    "    dependencies = {\n",
    "        'torch': 'PyTorch', 'transformers': 'Hugging Face Transformers',\n",
    "        'PIL': 'Pillow', 'Levenshtein': 'python-Levenshtein', 'sentencepiece': 'SentencePiece'\n",
    "    }\n",
    "    missing = []\n",
    "    for module, name in dependencies.items():\n",
    "        try:\n",
    "            __import__(module)\n",
    "            print(f\"[OK] {name} installed\")\n",
    "        except ImportError:\n",
    "            print(f\"[X]  {name} NOT installed\")\n",
    "            missing.append(name)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"\\n[!] Missing: {', '.join(missing)}\")\n",
    "        return False\n",
    "    print(\"\\n[OK] All dependencies installed!\")\n",
    "    return True\n",
    "\n",
    "def check_gpu():\n",
    "    print_section(\"CHECKING GPU\")\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[OK] GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            return True\n",
    "        print(\"[X]  No GPU detected (Expected for local check)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[X]  Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_paths():\n",
    "    print_section(\"CHECKING PATHS\")\n",
    "    global IMAGES_DIR, JSONL_ROOT_DIR\n",
    "    valid = True\n",
    "    \n",
    "    # --- תיקון לבעיית הכפילות בווינדוס ---\n",
    "    if os.path.exists(IMAGES_DIR):\n",
    "        image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.tiff', '*.bmp']\n",
    "        unique_images = set()\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            # חיפוש גם לקטן וגם לגדול\n",
    "            files = glob.glob(os.path.join(IMAGES_DIR, ext)) + glob.glob(os.path.join(IMAGES_DIR, ext.upper()))\n",
    "            for f in files:\n",
    "                # שימוש ב-abspath מבטיח שכל קובץ נספר פעם אחת בלבד\n",
    "                unique_images.add(os.path.abspath(f))\n",
    "        \n",
    "        count = len(unique_images)\n",
    "        print(f\"[OK] Images dir exists\")\n",
    "        print(f\"     Found {count} unique images\")\n",
    "        \n",
    "        if count == 0: \n",
    "            print(\"  [!] WARNING: No images found!\")\n",
    "            valid = False\n",
    "    else:\n",
    "        print(f\"[X]  Images dir NOT found: {IMAGES_DIR}\")\n",
    "        valid = False\n",
    "\n",
    "    if os.path.exists(JSONL_ROOT_DIR):\n",
    "        print(f\"[OK] JSONL dir exists\")\n",
    "    else:\n",
    "        print(f\"[X]  JSONL dir NOT found: {JSONL_ROOT_DIR}\")\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "def check_jsonl_content():\n",
    "    \"\"\"Combined check for JSON validity and Image Mapping\"\"\"\n",
    "    print_section(\"CHECKING JSONL CONTENT & MAPPING\")\n",
    "    global JSONL_ROOT_DIR, IMAGES_DIR\n",
    "    \n",
    "    files = glob.glob(os.path.join(JSONL_ROOT_DIR, \"*.jsonl\"))\n",
    "    if not files:\n",
    "        files = glob.glob(os.path.join(JSONL_ROOT_DIR, \"**\", \"*.jsonl\"), recursive=True)\n",
    "        \n",
    "    if not files:\n",
    "        print(f\"[X] No .jsonl files found\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"[OK] Found {len(files)} JSONL files\")\n",
    "    \n",
    "    total_entries = 0\n",
    "    total_mapped = 0\n",
    "    total_missing = 0\n",
    "    total_json_errors = 0\n",
    "    \n",
    "    for f in files:\n",
    "        print(f\"\\nScanning: {os.path.basename(f)}\")\n",
    "        file_entries = 0\n",
    "        \n",
    "        with open(f, 'r', encoding='utf-8') as fp:\n",
    "            for i, line in enumerate(fp, 1):\n",
    "                if not line.strip(): continue\n",
    "                \n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    file_entries += 1\n",
    "                    \n",
    "                    # 1. Check for Image Field (כולל התיקון ל-sample_id)\n",
    "                    image_name = None\n",
    "                    \n",
    "                    if 'sample_id' in entry:\n",
    "                        sid = entry['sample_id']\n",
    "                        # בדיקה חכמה: קודם jpg ואז png\n",
    "                        if os.path.exists(os.path.join(IMAGES_DIR, f\"datasheet_{sid}.jpg\")):\n",
    "                            image_name = f\"datasheet_{sid}.jpg\"\n",
    "                        elif os.path.exists(os.path.join(IMAGES_DIR, f\"datasheet_{sid}.png\")):\n",
    "                            image_name = f\"datasheet_{sid}.png\"\n",
    "                        else:\n",
    "                            # אם לא נמצא, נרשום את מה שהיה אמור להיות (כדי לספור אותו כחסר)\n",
    "                            image_name = f\"datasheet_{sid}.jpg\" \n",
    "                            \n",
    "                    elif 'image_path' in entry: image_name = os.path.basename(entry['image_path'])\n",
    "                    elif 'image' in entry: image_name = os.path.basename(entry['image'])\n",
    "                    elif 'filename' in entry: image_name = os.path.basename(entry['filename'])\n",
    "                    \n",
    "                    if not image_name:\n",
    "                        # שגיאה אמיתית: אין שום דרך לדעת מה התמונה\n",
    "                        print(f\"  [X] Line {i}: No 'sample_id' or image path found\")\n",
    "                        total_json_errors += 1\n",
    "                        continue\n",
    "\n",
    "                    # 2. Verify Image Exists\n",
    "                    full_path = os.path.join(IMAGES_DIR, image_name)\n",
    "                    if os.path.exists(full_path):\n",
    "                        total_mapped += 1\n",
    "                    else:\n",
    "                        if total_missing < 5: # מדפיס רק את ה-5 הראשונים החסרים\n",
    "                            print(f\"  [!] Missing image: {image_name}\")\n",
    "                        total_missing += 1\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"  [X] Line {i}: Invalid JSON\")\n",
    "                    total_json_errors += 1\n",
    "        \n",
    "        total_entries += file_entries\n",
    "        print(f\"  Entries: {file_entries}\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TOTAL SUMMARY:\")\n",
    "    print(f\"  Total JSON Entries: {total_entries}\")\n",
    "    print(f\"  Successfully Mapped: {total_mapped}\")\n",
    "    print(f\"  Missing Images:      {total_missing}\")\n",
    "    print(f\"  JSON/Format Errors:  {total_json_errors}\")\n",
    "    \n",
    "    # לוגיקה לעבור/לא לעבור\n",
    "    if total_entries == 0:\n",
    "        print(\"\\n[X] No data found.\")\n",
    "        return False\n",
    "        \n",
    "    if total_mapped == 0:\n",
    "        print(\"\\n[X] Critical: 0 images mapped. Check folder paths.\")\n",
    "        return False\n",
    "        \n",
    "    if total_missing > 0:\n",
    "        print(f\"\\n[!] Warning: {total_missing} images are missing.\")\n",
    "        print(\"    The training script will simply skip these. This is acceptable.\")\n",
    "        \n",
    "    print(f\"\\n[OK] Data validation passed ({total_mapped} valid samples ready)\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--images_dir\", required=True)\n",
    "    parser.add_argument(\"--jsonl_dir\", required=True)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    global IMAGES_DIR, JSONL_ROOT_DIR\n",
    "    IMAGES_DIR = args.images_dir\n",
    "    JSONL_ROOT_DIR = args.jsonl_dir\n",
    "    \n",
    "    checks = [\n",
    "        (\"Dependencies\", check_dependencies),\n",
    "        (\"GPU\", check_gpu),\n",
    "        (\"Paths\", check_paths),\n",
    "        (\"Content & Mapping\", check_jsonl_content)\n",
    "    ]\n",
    "    \n",
    "    all_pass = True\n",
    "    for name, func in checks:\n",
    "        try:\n",
    "            if not func(): all_pass = False\n",
    "        except Exception as e:\n",
    "            print(f\"[X] Crash in {name}: {e}\")\n",
    "            all_pass = False\n",
    "            \n",
    "    print_section(\"FINAL STATUS\")\n",
    "    if all_pass:\n",
    "        print(\"[OK] READY TO TRAIN\")\n",
    "        print(\"     (Remember to run on a machine with GPU!)\")\n",
    "    else:\n",
    "        print(\"[X] FIX ERRORS ABOVE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "הרצת הבדיקה"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKING DEPENDENCIES\n",
      "================================================================================\n",
      "\n",
      "[OK] PyTorch installed\n",
      "[OK] Hugging Face Transformers installed\n",
      "[OK] Pillow installed\n",
      "[OK] python-Levenshtein installed\n",
      "[OK] SentencePiece installed\n",
      "\n",
      "[OK] All dependencies installed!\n",
      "\n",
      "================================================================================\n",
      "CHECKING GPU\n",
      "================================================================================\n",
      "\n",
      "[X]  No GPU detected (Expected for local check)\n",
      "\n",
      "================================================================================\n",
      "CHECKING PATHS\n",
      "================================================================================\n",
      "\n",
      "[OK] Images dir exists\n",
      "     Found 6737 unique images\n",
      "[OK] JSONL dir exists\n",
      "\n",
      "================================================================================\n",
      "CHECKING JSONL CONTENT & MAPPING\n",
      "================================================================================\n",
      "\n",
      "[OK] Found 4 JSONL files\n",
      "\n",
      "Scanning: production_20260208_001537.jsonl\n",
      "  Entries: 2000\n",
      "\n",
      "Scanning: production_20260208_035545.jsonl\n",
      "  Entries: 2000\n",
      "\n",
      "Scanning: production_20260208_070204.jsonl\n",
      "  Entries: 2000\n",
      "\n",
      "Scanning: production_20260208_090359.jsonl\n",
      "  [!] Missing image: datasheet_d0cf48ff.jpg\n",
      "  [!] Missing image: datasheet_d6495cf3.jpg\n",
      "  [!] Missing image: datasheet_36d8ac6f.jpg\n",
      "  [!] Missing image: datasheet_6af0fa73.jpg\n",
      "  [!] Missing image: datasheet_ec9e09e7.jpg\n",
      "  Entries: 755\n",
      "\n",
      "================================================================================\n",
      "TOTAL SUMMARY:\n",
      "  Total JSON Entries: 6755\n",
      "  Successfully Mapped: 6742\n",
      "  Missing Images:      13\n",
      "  JSON/Format Errors:  0\n",
      "\n",
      "[!] Warning: 13 images are missing.\n",
      "    The training script will simply skip these. This is acceptable.\n",
      "\n",
      "[OK] Data validation passed (6742 valid samples ready)\n",
      "\n",
      "================================================================================\n",
      "FINAL STATUS\n",
      "================================================================================\n",
      "\n",
      "[X] FIX ERRORS ABOVE\n"
     ]
    }
   ],
   "source": [
    "!python check_setup.py --images_dir \"C:\\Users\\nivsa\\Generation of Synthetic Training Data\\embedded\\final_dataset\\images\" --jsonl_dir \"C:\\Users\\nivsa\\Generation of Synthetic Training Data\\embedded\\final_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\"\"\"\n",
    "Production-Ready Donut Fine-tuning Script for Electronic Datasheets\n",
    "====================================================================\n",
    "Author: Senior Computer Vision & NLP Engineer\n",
    "Purpose: Fine-tune naver-clova-ix/donut-base on synthetic electronic datasheets\n",
    "         with dense tables, small text, technical units, and charts.\n",
    "\n",
    "Key Features:\n",
    "- Multi-JSONL file aggregation with robust path handling\n",
    "- Automatic special token discovery and injection\n",
    "- High-resolution image processing (1600x1200) for small text\n",
    "- Mixed precision training (FP16)\n",
    "- Levenshtein distance metric for evaluation\n",
    "- Stratified Splitting (Critical for balanced training)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    DonutProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "# from datasets import Dataset as HFDataset # Not strictly needed here\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration for training\"\"\"\n",
    "    \n",
    "    # Data paths - will be set via command line arguments\n",
    "    IMAGES_DIR = None\n",
    "    JSONL_ROOT_DIR = None\n",
    "    \n",
    "    # Model configuration\n",
    "    MODEL_NAME = \"naver-clova-ix/donut-base\"\n",
    "    OUTPUT_DIR = \"./donut-datasheets-finetuned\"\n",
    "    \n",
    "    # High-resolution settings for small text in datasheets\n",
    "    # (1600, 1200) is a sweet spot for 16GB-24GB GPUs\n",
    "    IMAGE_SIZE = (1600, 1200)  # (width, height)\n",
    "    \n",
    "    # Generation settings - long context for verbose datasheets\n",
    "    MAX_LENGTH = 1024\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 10\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # Data split\n",
    "    TRAIN_SPLIT_RATIO = 0.9\n",
    "    \n",
    "    # Evaluation\n",
    "    EVAL_STEPS = 500\n",
    "    SAVE_STEPS = 500\n",
    "    SAVE_TOTAL_LIMIT = 2\n",
    "    \n",
    "    # Seed for reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def load_all_jsonl_files(jsonl_root_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load and aggregate all JSONL files from the root directory.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading JSONL files from: {jsonl_root_dir}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Find all JSONL files using glob (handles Windows paths correctly)\n",
    "    jsonl_pattern = os.path.join(jsonl_root_dir, \"*.jsonl\")\n",
    "    jsonl_files = glob.glob(jsonl_pattern)\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        # Try recursive search\n",
    "        jsonl_pattern = os.path.join(jsonl_root_dir, \"**\", \"*.jsonl\")\n",
    "        jsonl_files = glob.glob(jsonl_pattern, recursive=True)\n",
    "    \n",
    "    print(f\"Found {len(jsonl_files)} JSONL file(s):\")\n",
    "    for jsonl_file in jsonl_files:\n",
    "        print(f\"  - {os.path.basename(jsonl_file)}\")\n",
    "    \n",
    "    if not jsonl_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No JSONL files found in {jsonl_root_dir}. \"\n",
    "            f\"Please check the path and ensure *.jsonl files exist.\"\n",
    "        )\n",
    "    \n",
    "    # Aggregate all entries\n",
    "    all_entries = []\n",
    "    \n",
    "    for jsonl_file in jsonl_files:\n",
    "        print(f\"\\nProcessing: {jsonl_file}\")\n",
    "        \n",
    "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "            file_entries = 0\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                    all_entries.append(entry)\n",
    "                    file_entries += 1\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"  WARNING: Skipping invalid JSON at line {line_num}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  Loaded {file_entries} entries\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Total entries loaded: {len(all_entries)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return all_entries\n",
    "\n",
    "\n",
    "def construct_image_paths(entries: List[Dict], images_dir: str) -> List[Dict]:\n",
    "    \"\"\"Construct full image paths from filenames in JSONL entries.\"\"\"\n",
    "    print(\"Constructing full image paths...\")\n",
    "    \n",
    "    processed_entries = []\n",
    "    missing_images = []\n",
    "    \n",
    "    for entry in entries:\n",
    "        image_name = None\n",
    "        \n",
    "        # --- Logic to find image name from sample_id ---\n",
    "        if 'sample_id' in entry:\n",
    "            sid = entry['sample_id']\n",
    "            # Try jpg first\n",
    "            if os.path.exists(os.path.join(images_dir, f\"datasheet_{sid}.jpg\")):\n",
    "                image_name = f\"datasheet_{sid}.jpg\"\n",
    "            elif os.path.exists(os.path.join(images_dir, f\"datasheet_{sid}.png\")):\n",
    "                image_name = f\"datasheet_{sid}.png\"\n",
    "            else:\n",
    "                image_name = f\"datasheet_{sid}.jpg\" # Fallback\n",
    "        \n",
    "        # Support legacy keys\n",
    "        elif 'image_path' in entry: image_name = os.path.basename(entry['image_path'])\n",
    "        elif 'image' in entry: image_name = os.path.basename(entry['image'])\n",
    "        elif 'filename' in entry: image_name = os.path.basename(entry['filename'])\n",
    "        \n",
    "        if not image_name:\n",
    "            # print(f\"  WARNING: Entry missing image filename\") # Too verbose\n",
    "            continue\n",
    "        \n",
    "        # Construct full path\n",
    "        full_image_path = os.path.join(images_dir, image_name)\n",
    "        \n",
    "        # Verify image exists\n",
    "        if not os.path.exists(full_image_path):\n",
    "            missing_images.append(full_image_path)\n",
    "            continue\n",
    "        \n",
    "        # Create processed entry\n",
    "        processed_entry = {\n",
    "            'image_path': full_image_path,\n",
    "            'ground_truth': entry.get('ground_truth', {}),\n",
    "            # Keep original fields for stratification\n",
    "            'component_type': entry.get('component_type', 'unknown') \n",
    "        }\n",
    "        \n",
    "        processed_entries.append(processed_entry)\n",
    "    \n",
    "    print(f\"  Successfully processed: {len(processed_entries)} entries\")\n",
    "    \n",
    "    if missing_images:\n",
    "        print(f\"  WARNING: {len(missing_images)} images not found (first 5):\")\n",
    "        for img_path in missing_images[:5]:\n",
    "            print(f\"    - {img_path}\")\n",
    "    \n",
    "    return processed_entries\n",
    "\n",
    "\n",
    "def extract_unique_keys(entries: List[Dict]) -> List[str]:\n",
    "    \"\"\"Extract all unique JSON keys from ground truth data for special tokens.\"\"\"\n",
    "    print(\"\\nExtracting unique keys from ground truth data...\")\n",
    "    \n",
    "    unique_keys = set()\n",
    "    \n",
    "    def extract_keys_recursive(obj: Any):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                unique_keys.add(key)\n",
    "                extract_keys_recursive(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract_keys_recursive(item)\n",
    "    \n",
    "    for entry in entries:\n",
    "        ground_truth = entry.get('ground_truth', {})\n",
    "        extract_keys_recursive(ground_truth)\n",
    "    \n",
    "    sorted_keys = sorted(list(unique_keys))\n",
    "    \n",
    "    print(f\"  Found {len(sorted_keys)} unique keys:\")\n",
    "    for key in sorted_keys[:20]:\n",
    "        print(f\"    - {key}\")\n",
    "    if len(sorted_keys) > 20:\n",
    "        print(f\"    ... and {len(sorted_keys) - 20} more\")\n",
    "    \n",
    "    return sorted_keys\n",
    "\n",
    "\n",
    "def split_train_val_stratified(\n",
    "    entries: List[Dict], \n",
    "    train_ratio: float = 0.9, \n",
    "    seed: int = 42,\n",
    "    stratify_key: str = \"component_type\"\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets with STRATIFICATION.\n",
    "    Ensures rare component types are represented in both sets.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSplitting data with STRATIFICATION on '{stratify_key}'\")\n",
    "    print(f\"  Train ratio: {train_ratio*100:.0f}%\")\n",
    "    print(f\"  Validation ratio: {(1-train_ratio)*100:.0f}%\")\n",
    "    \n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Group entries by component type\n",
    "    groups = {}\n",
    "    entries_without_key = []\n",
    "    \n",
    "    for entry in entries:\n",
    "        # Check both top-level and inside ground_truth\n",
    "        strat_value = entry.get(stratify_key) or entry.get('ground_truth', {}).get(stratify_key)\n",
    "        \n",
    "        if strat_value:\n",
    "            if strat_value not in groups:\n",
    "                groups[strat_value] = []\n",
    "            groups[strat_value].append(entry)\n",
    "        else:\n",
    "            entries_without_key.append(entry)\n",
    "    \n",
    "    print(f\"\\nFound {len(groups)} categories:\")\n",
    "    for category, items in sorted(groups.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        print(f\"  - {category}: {len(items)} samples\")\n",
    "    \n",
    "    if entries_without_key:\n",
    "        print(f\"  - [No {stratify_key}]: {len(entries_without_key)} samples\")\n",
    "    \n",
    "    train_entries = []\n",
    "    val_entries = []\n",
    "    \n",
    "    # Split each group\n",
    "    for category, items in groups.items():\n",
    "        random.shuffle(items)\n",
    "        split_idx = max(1, int(len(items) * train_ratio)) # Ensure at least 1 in train\n",
    "        \n",
    "        # If we have only 1 sample, put it in train (otherwise validation crashes)\n",
    "        if len(items) == 1:\n",
    "            train_entries.extend(items)\n",
    "        else:\n",
    "            train_entries.extend(items[:split_idx])\n",
    "            val_entries.extend(items[split_idx:])\n",
    "        \n",
    "        print(f\"  {category}: {len(items[:split_idx]) if len(items)>1 else 1} train, {len(items[split_idx:]) if len(items)>1 else 0} val\")\n",
    "    \n",
    "    # Handle unlabeled\n",
    "    if entries_without_key:\n",
    "        random.shuffle(entries_without_key)\n",
    "        split_idx = int(len(entries_without_key) * train_ratio)\n",
    "        train_entries.extend(entries_without_key[:split_idx])\n",
    "        val_entries.extend(entries_without_key[split_idx:])\n",
    "    \n",
    "    random.shuffle(train_entries)\n",
    "    random.shuffle(val_entries)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FINAL SPLIT:\")\n",
    "    print(f\"  Training samples: {len(train_entries)}\")\n",
    "    print(f\"  Validation samples: {len(val_entries)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return train_entries, val_entries\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class DonutDatasheetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        entries: List[Dict],\n",
    "        processor: DonutProcessor,\n",
    "        max_length: int = 1024,\n",
    "        task_start_token: str = \"<s_datasheet>\",\n",
    "    ):\n",
    "        self.entries = entries\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.task_start_token = task_start_token\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.entries)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        entry = self.entries[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image_path = entry['image_path']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            image = Image.new(\"RGB\", (100, 100), color=(255, 255, 255))\n",
    "        \n",
    "        # Process image\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        \n",
    "        # Prepare ground truth\n",
    "        ground_truth = entry['ground_truth']\n",
    "        ground_truth_json = json.dumps(ground_truth, ensure_ascii=False)\n",
    "        \n",
    "        # Create target sequence\n",
    "        target_sequence = f\"{self.task_start_token}{ground_truth_json}</s>\"\n",
    "        \n",
    "        # Tokenize\n",
    "        labels = self.processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.squeeze()\n",
    "        \n",
    "        # Ignore padding in loss\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_metrics(pred, processor: DonutProcessor) -> Dict[str, float]:\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    labels = [[token if token != -100 else processor.tokenizer.pad_token_id for token in label] for label in labels]\n",
    "    \n",
    "    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    total_distance = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for pred_str, label_str in zip(decoded_preds, decoded_labels):\n",
    "        distance = Levenshtein.distance(pred_str, label_str)\n",
    "        total_distance += distance\n",
    "        total_length += len(label_str)\n",
    "    \n",
    "    normalized_distance = total_distance / max(total_length, 1)\n",
    "    \n",
    "    return {\"normalized_edit_distance\": normalized_distance}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--images_dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--jsonl_dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./donut-datasheets-finetuned\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--max_samples\", type=int, default=None)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    Config.IMAGES_DIR = args.images_dir\n",
    "    Config.JSONL_ROOT_DIR = args.jsonl_dir\n",
    "    Config.OUTPUT_DIR = args.output_dir\n",
    "    Config.TRAIN_BATCH_SIZE = args.batch_size\n",
    "    Config.NUM_EPOCHS = args.epochs\n",
    "    Config.LEARNING_RATE = args.learning_rate\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DONUT FINE-TUNING (Production Version)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    all_entries = load_all_jsonl_files(Config.JSONL_ROOT_DIR)\n",
    "    all_entries = construct_image_paths(all_entries, Config.IMAGES_DIR)\n",
    "    \n",
    "    if args.max_samples:\n",
    "        print(f\"\\n[!] Limiting dataset to {args.max_samples} samples for testing\")\n",
    "        random.shuffle(all_entries)\n",
    "        all_entries = all_entries[:args.max_samples]\n",
    "    \n",
    "    # 2. Prepare Tokens\n",
    "    unique_keys = extract_unique_keys(all_entries)\n",
    "    special_tokens = [f\"<s_{key}>\" for key in unique_keys]\n",
    "    new_tokens = [\"<s_datasheet>\"] + special_tokens\n",
    "    \n",
    "    # 3. Stratified Split\n",
    "    train_entries, val_entries = split_train_val_stratified(\n",
    "        all_entries, \n",
    "        train_ratio=Config.TRAIN_SPLIT_RATIO,\n",
    "        seed=Config.SEED\n",
    "    )\n",
    "    \n",
    "    # Validation check to prevent crash\n",
    "    if len(val_entries) == 0:\n",
    "        print(\"[!] Warning: Validation set empty. Moving 1 sample from train to val.\")\n",
    "        val_entries = [train_entries.pop()]\n",
    "\n",
    "    # 4. Model & Processor\n",
    "    print(\"\\nLoading model...\")\n",
    "    processor = DonutProcessor.from_pretrained(Config.MODEL_NAME)\n",
    "    processor.image_processor.size = {\"height\": Config.IMAGE_SIZE[1], \"width\": Config.IMAGE_SIZE[0]}\n",
    "    processor.image_processor.do_align_long_axis = False\n",
    "    \n",
    "    processor.tokenizer.add_tokens(new_tokens)\n",
    "    \n",
    "    model = VisionEncoderDecoderModel.from_pretrained(Config.MODEL_NAME)\n",
    "    model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "    model.config.max_length = Config.MAX_LENGTH\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<s_datasheet>\"])[0]\n",
    "    \n",
    "    # 5. Datasets\n",
    "    train_ds = DonutDatasheetDataset(train_entries, processor, Config.MAX_LENGTH)\n",
    "    val_ds = DonutDatasheetDataset(val_entries, processor, Config.MAX_LENGTH)\n",
    "    \n",
    "    # 6. Training Arguments (FIXED)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        num_train_epochs=Config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=Config.TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=Config.TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        warmup_steps=Config.WARMUP_STEPS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        \n",
    "        # --- THE FIX ---\n",
    "        eval_strategy=\"steps\",  # Was evaluation_strategy\n",
    "        eval_steps=Config.EVAL_STEPS,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=Config.SAVE_STEPS,\n",
    "        save_total_limit=Config.SAVE_TOTAL_LIMIT,\n",
    "        # ---------------\n",
    "        \n",
    "        predict_with_generate=True,\n",
    "        logging_steps=100,\n",
    "        remove_unused_columns=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"normalized_edit_distance\",\n",
    "        greater_is_better=False\n",
    "    )\n",
    "    \n",
    "    # 7. Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=lambda p: compute_metrics(p, processor)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"\\nSaving model to {Config.OUTPUT_DIR}\")\n",
    "    trainer.save_model(Config.OUTPUT_DIR)\n",
    "    processor.save_pretrained(Config.OUTPUT_DIR)\n",
    "    print(\"DONE!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "הרצת קוד האימון"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --images_dir \"C:\\Users\\nivsa\\Generation of Synthetic Training Data\\embedded\\final_dataset\\images\" --jsonl_dir \"C:\\Users\\nivsa\\Generation of Synthetic Training Data\\embedded\\final_dataset\" --batch_size 2 --epochs 1 --max_samples 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בדיקת התוצאות"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Donut Inference for Jupyter Notebook\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# --- Imports ---\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION (Change these paths!)\n",
    "# ============================================================================\n",
    "\n",
    "# Path to your fine-tuned model folder (where train.py saved it)\n",
    "MODEL_PATH = \"./donut-datasheets-finetuned\"\n",
    "\n",
    "# Path to the image you want to test\n",
    "IMAGE_PATH = r\"C:\\Users\\nivsa\\Generation of Synthetic Training Data\\embedded\\final_dataset\\images\\datasheet_00eb2cd6.jpg\"\n",
    "\n",
    "# Device (cuda or cpu)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_model(model_path, device=\"cuda\"):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    try:\n",
    "        processor = DonutProcessor.from_pretrained(model_path)\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "    except OSError as e:\n",
    "        print(f\"[X] Error: Could not load model from {model_path}\")\n",
    "        print(f\"  Details: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"[OK] Model loaded on {device}\")\n",
    "    print(f\"[OK] Input Resolution: {processor.image_processor.size}\")\n",
    "    return model, processor\n",
    "\n",
    "def extract_json(image_path, model, processor, device=\"cuda\"):\n",
    "    print(f\"\\nProcessing: {os.path.basename(image_path)}\")\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "    # Display image in notebook\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Prepare input\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    task_prompt = \"<s_datasheet>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Generate\n",
    "    print(\"Generating output...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=1024,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    seq = processor.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "    clean_seq = seq.replace(task_prompt, \"\").strip()\n",
    "    \n",
    "    # Try parsing JSON\n",
    "    try:\n",
    "        return json.loads(clean_seq)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: Try to find JSON object in text\n",
    "        match = re.search(r'\\{.*\\}', clean_seq, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(0))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Fallback: Fix missing braces\n",
    "        if clean_seq.startswith(\"{\") and not clean_seq.endswith(\"}\"):\n",
    "            try:\n",
    "                return json.loads(clean_seq + \"}\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return {\"raw_output\": clean_seq, \"error\": \"Failed to parse JSON\"}\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Load Model (Only runs once if you keep the cell output)\n",
    "if 'model' not in globals():\n",
    "    model, processor = load_model(MODEL_PATH, DEVICE)\n",
    "\n",
    "# 2. Run Inference\n",
    "if model:\n",
    "    result = extract_json(IMAGE_PATH, model, processor, DEVICE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTION RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
